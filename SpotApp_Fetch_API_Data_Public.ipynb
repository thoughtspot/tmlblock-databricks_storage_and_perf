{"cells":[{"cell_type":"code","source":["import requests\nfrom datetime import date, datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import from_unixtime, lit, json_tuple\n#from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, DateType\nfrom delta.tables import *\nimport time\nfrom base64 import b64encode\n\n\nimport io\nimport pandas as pd\nfrom pandas import json_normalize\nimport json\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Imports","showTitle":true,"inputWidgets":{},"nuid":"70737216-e6dc-475d-9aac-7048390781fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# If you want to run this notebook yourself, you need to create a Databricks personal access token,\n# https://docs.databricks.com/sql/user/security/personal-access-tokens.html\n# store it using our secrets API, and pass it in through the Spark config, such as this:\n# spark.pat_token {{secrets/query_history_etl/user}}, or Azure Keyvault.\n\nWORKSPACE_HOST = 'https://UPDATE_ENTERHOSTID.cloud.databricks.com' #https://docs.databricks.com/administration-guide/account-settings/billable-usage-download-api.html\nENDPOINTS_URL = \"{0}/api/2.0/sql/endpoints\".format(WORKSPACE_HOST) #https://docs.databricks.com/sql/api/sql-endpoints.html\n\n#QUERY API PARAMETERS\nMAX_RESULTS_PER_PAGE = 25000\nMAX_PAGES_PER_RUN = 500\nQUERIES_URL = \"{0}/api/2.0/sql/history/queries\".format(WORKSPACE_HOST) #https://docs.databricks.com/sql/api/query-history.html\n\n\n# We will fetch all queries that were started between this number of hours ago, and now()\n# Queries that are running for longer than this will not be updated.\n# Can be set to a much higher number when backfilling data, for example when this Job didn't\n# run for a while.\nNUM_HOURS_TO_UPDATE = 24\nSTART_DATE = datetime.now() - timedelta(hours=NUM_HOURS_TO_UPDATE)\n\n#ACCOUNTS API\nACCOUNTS_HOST = 'https://accounts.cloud.databricks.com'\nACCOUNT_ID = 'UPDATE_ACCOUNT_ID' #available from admin portal\nSTART_MONTH = (datetime.today() - relativedelta(months=NUM_MONTHS_TO_UPDATE)).strftime('%Y-%m') # i.e. '2022-01'\nEND_MONTH = datetime.today().strftime('%Y-%m')\nPERSONAL_DATA = True\n\nBILLING_URL = \"{0}/api/2.0/accounts/{1}/usage/download?start_month={2}&end_month={3}&personal_data={4}\".format(ACCOUNTS_HOST,ACCOUNT_ID,START_MONTH, END_MONTH, PERSONAL_DATA)\n\n#DATABASE_NAME = \"query_history_etl\"\nDATABASE_NAME = \"spotapps\"\nENDPOINTS_TABLE_NAME = \"endpoints\"\nQUERIES_TABLE_NAME = \"queries\"\nBILLING_TABLE_NAME = \"billing\"\n\n#Databricks secrets API\n#https://docs.databricks.com/dev-tools/api/latest/authentication.html\n\n#auth_header = {\"Authorization\" : \"Bearer \" + spark.conf.get(\"spark.pat_token\")}\n#Azure KeyVault\n#auth_header = {\"Authorization\" : \"Bearer \" + dbutils.secrets.get(scope = \"<scope-name>\", key = \"<key-name>\")}\n\n#hardcoded authorisation token\nauth_header = {\"Authorization\" : \"Bearer UPDATE_AUTHORISATION_TOKEN\"}\n\n#basic auth required for billing API\nauth_header_basic = {\n    \"Authorization\": \"Basic {}\".format(\n        b64encode(bytes(\"UPDATE_USER_EMAIL:UPDATE_USER_PWD\", \"utf-8\")).decode(\"ascii\")\n    )\n}\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Constants","showTitle":true,"inputWidgets":{},"nuid":"1a1fc132-66db-49af-8ead-a210603873ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["def check_table_exist(db_tbl_name):\n    table_exist = False\n    try:\n        spark.read.table(db_tbl_name) # Check if spark can read the table\n        table_exist = True        \n    except:\n        pass\n    return table_exist\n  \ndef current_time_in_millis():\n    return round(time.time() * 1000)\n  \ndef get_boolean_keys(arrays):\n  # A quirk in Python's and Spark's handling of JSON booleans requires us to converting True and False to true and false\n  boolean_keys_to_convert = []\n  for array in arrays:\n    for key in array.keys():\n      if type(array[key]) is bool:\n        boolean_keys_to_convert.append(key)\n  #print(boolean_keys_to_convert)\n  return boolean_keys_to_convert"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Functions definition","showTitle":true,"inputWidgets":{},"nuid":"99395ff0-457c-4228-b6e5-2bd32c34eb9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["notebook_start_execution_time = current_time_in_millis()\n\nspark.sql(\"CREATE DATABASE IF NOT EXISTS {}\".format(DATABASE_NAME))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Start Process","showTitle":true,"inputWidgets":{},"nuid":"e57d2de6-d21c-404e-873d-c6a7b4ec7b36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: DataFrame[]</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["response = requests.get(ENDPOINTS_URL, headers=auth_header)\n\nif response.status_code != 200:\n  raise Exception(response.text)\nresponse_json = response.json()\n\nendpoints_json = response_json[\"endpoints\"]\n\n# A quirk in Python's and Spark's handling of JSON booleans requires us to converting True and False to true and false\nboolean_keys_to_convert = set(get_boolean_keys(endpoints_json))\n\nfor endpoint_json in endpoints_json:\n  for key in boolean_keys_to_convert:\n    endpoint_json[key] = str(endpoint_json[key]).lower()\n\nendpoints = spark.read.json(sc.parallelize(endpoints_json))\ndisplay(endpoints)\n\nendpoints.write.format(\"delta\").option(\"overwriteSchema\", \"true\").mode(\"overwrite\").saveAsTable(DATABASE_NAME + \".\" + ENDPOINTS_TABLE_NAME)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Fetch from SQL Endpoint API","showTitle":true,"inputWidgets":{},"nuid":"610b6db1-b043-48eb-a78c-a0132434ae5e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["true",10,"Small",3990145733842747,"russell.ratshin@thoughtspot.com","true","false",["HEALTHY"],"07bccb401afcfe0c","jdbc:spark://dbc-3472b2da-8a4e.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/endpoints/07bccb401afcfe0c;",1,1,"PMMSQLE",0,1,["dbc-3472b2da-8a4e.cloud.databricks.com","/sql/1.0/endpoints/07bccb401afcfe0c",443,"https"],"SMALL","COST_OPTIMIZED","RUNNING",[[["Application","Databricks"]]]],["true",10,"2X-Small",2987448525455285,"todd.beauchene@thoughtspot.com","true","false",null,"6a882f6a859e0002","jdbc:spark://dbc-3472b2da-8a4e.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/endpoints/6a882f6a859e0002;",1,1,"SEDEMO",0,0,["dbc-3472b2da-8a4e.cloud.databricks.com","/sql/1.0/endpoints/6a882f6a859e0002",443,"https"],"XXSMALL","COST_OPTIMIZED","STOPPED",[null]],["true",10,"2X-Small",2849272732075973,"kapil.khurana@thoughtspot.com","true","false",null,"0058f808defd479e","jdbc:spark://dbc-3472b2da-8a4e.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/endpoints/0058f808defd479e;",1,1,"SQL_DEV_TESTING",0,0,["dbc-3472b2da-8a4e.cloud.databricks.com","/sql/1.0/endpoints/0058f808defd479e",443,"https"],"XXSMALL","COST_OPTIMIZED","STOPPED",[null]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"auto_resume","type":"\"string\"","metadata":"{}"},{"name":"auto_stop_mins","type":"\"long\"","metadata":"{}"},{"name":"cluster_size","type":"\"string\"","metadata":"{}"},{"name":"creator_id","type":"\"long\"","metadata":"{}"},{"name":"creator_name","type":"\"string\"","metadata":"{}"},{"name":"enable_photon","type":"\"string\"","metadata":"{}"},{"name":"enable_serverless_compute","type":"\"string\"","metadata":"{}"},{"name":"health","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"status\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"jdbc_url","type":"\"string\"","metadata":"{}"},{"name":"max_num_clusters","type":"\"long\"","metadata":"{}"},{"name":"min_num_clusters","type":"\"long\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"num_active_sessions","type":"\"long\"","metadata":"{}"},{"name":"num_clusters","type":"\"long\"","metadata":"{}"},{"name":"odbc_params","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"hostname\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"port\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"protocol\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"size","type":"\"string\"","metadata":"{}"},{"name":"spot_instance_policy","type":"\"string\"","metadata":"{}"},{"name":"state","type":"\"string\"","metadata":"{}"},{"name":"tags","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"custom_tags\",\"type\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"key\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"value\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>auto_resume</th><th>auto_stop_mins</th><th>cluster_size</th><th>creator_id</th><th>creator_name</th><th>enable_photon</th><th>enable_serverless_compute</th><th>health</th><th>id</th><th>jdbc_url</th><th>max_num_clusters</th><th>min_num_clusters</th><th>name</th><th>num_active_sessions</th><th>num_clusters</th><th>odbc_params</th><th>size</th><th>spot_instance_policy</th><th>state</th><th>tags</th></tr></thead><tbody><tr><td>true</td><td>10</td><td>Small</td><td>3990145733842747</td><td>russell.ratshin@thoughtspot.com</td><td>true</td><td>false</td><td>List(HEALTHY)</td><td>07bccb401afcfe0c</td><td>jdbc:spark://dbc-3472b2da-8a4e.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/endpoints/07bccb401afcfe0c;</td><td>1</td><td>1</td><td>PMMSQLE</td><td>0</td><td>1</td><td>List(dbc-3472b2da-8a4e.cloud.databricks.com, /sql/1.0/endpoints/07bccb401afcfe0c, 443, https)</td><td>SMALL</td><td>COST_OPTIMIZED</td><td>RUNNING</td><td>List(List(List(Application, Databricks)))</td></tr><tr><td>true</td><td>10</td><td>2X-Small</td><td>2987448525455285</td><td>todd.beauchene@thoughtspot.com</td><td>true</td><td>false</td><td>null</td><td>6a882f6a859e0002</td><td>jdbc:spark://dbc-3472b2da-8a4e.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/endpoints/6a882f6a859e0002;</td><td>1</td><td>1</td><td>SEDEMO</td><td>0</td><td>0</td><td>List(dbc-3472b2da-8a4e.cloud.databricks.com, /sql/1.0/endpoints/6a882f6a859e0002, 443, https)</td><td>XXSMALL</td><td>COST_OPTIMIZED</td><td>STOPPED</td><td>List(null)</td></tr><tr><td>true</td><td>10</td><td>2X-Small</td><td>2849272732075973</td><td>kapil.khurana@thoughtspot.com</td><td>true</td><td>false</td><td>null</td><td>0058f808defd479e</td><td>jdbc:spark://dbc-3472b2da-8a4e.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/endpoints/0058f808defd479e;</td><td>1</td><td>1</td><td>SQL_DEV_TESTING</td><td>0</td><td>0</td><td>List(dbc-3472b2da-8a4e.cloud.databricks.com, /sql/1.0/endpoints/0058f808defd479e, 443, https)</td><td>XXSMALL</td><td>COST_OPTIMIZED</td><td>STOPPED</td><td>List(null)</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["START_DATE = datetime.now() - timedelta(hours=NUM_HOURS_TO_UPDATE)\nstart_time_ms = START_DATE.timestamp() * 1000\nend_time_ms = datetime.now().timestamp() * 1000\n\nnext_page_token = None\nhas_next_page = True\npages_fetched = 0\n\nwhile (has_next_page and pages_fetched < MAX_PAGES_PER_RUN):\n  print(\"Starting to fetch page \" + str(pages_fetched))\n  pages_fetched += 1\n  if next_page_token:\n    # Can not set filters after the first page\n    request_parameters = {\n      \"max_results\": MAX_RESULTS_PER_PAGE,\n      \"page_token\": next_page_token,\n      \"include_metrics\": True\n    }\n  else:\n    request_parameters = {\n      \"max_results\": MAX_RESULTS_PER_PAGE,\n      \"filter_by\": {\"query_start_time_range\": {\"start_time_ms\": start_time_ms, \"end_time_ms\": end_time_ms}},\n      \"include_metrics\": True\n    }\n\n  print (\"Request parameters: \" + str(request_parameters))\n  \n  response = requests.get(QUERIES_URL, headers=auth_header, json=request_parameters)\n  if response.status_code != 200:\n    raise Exception(response.text)\n  response_json = response.json()\n  next_page_token = response_json[\"next_page_token\"]\n  has_next_page = response_json[\"has_next_page\"]\n  \n  boolean_keys_to_convert = set(get_boolean_keys(response_json[\"res\"]))\n  for array_to_process in response_json[\"res\"]:\n    for key in boolean_keys_to_convert:\n      array_to_process[key] = str(array_to_process[key]).lower()\n  \n  #unable to flatten metrics json with spark\n  #query_results = spark.read.json(sc.parallelize(response_json[\"res\"]))\n  \n  #normalise the json into a pandas datafram\n  query_results_df = json_normalize(response_json[\"res\"])\n  #convert pandas df to spark\n  query_results = spark.createDataFrame(query_results_df)\n\n  # create date time fields from unixtime(ms)\n  query_results_clean = query_results \\\n    .withColumn(\"query_start_time\", from_unixtime(query_results.query_start_time_ms / 1000, 'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\")) \\\n    .withColumn(\"query_end_time\", from_unixtime(query_results.query_end_time_ms / 1000, 'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n  \n  # The error_message column is not present in the REST API response when none of the queries failed.\n  # In that case we add it as an empty column, since otherwise the Delta merge would fail in schema\n  # validation\n  if \"error_message\" not in query_results_clean.columns:\n    query_results_clean = query_results_clean.withColumn(\"error_message\", lit(\"\"))\n  \n  # was getting ad hoc error on merge with _corrupt_record, added to dataset as empty string\n  if \"_corrupt_record\" not in query_results_clean.columns:\n    query_results_clean = query_results_clean.withColumn(\"_corrupt_record\", lit(\"\"))\n  \n  if not check_table_exist(db_tbl_name=\"{0}.{1}\".format(DATABASE_NAME, QUERIES_TABLE_NAME)):\n    # TODO: Probably makes sense to partition and/or Z-ORDER this table.\n    query_results_clean.write.format(\"delta\").saveAsTable(\"{0}.{1}\".format(DATABASE_NAME, QUERIES_TABLE_NAME)) \n  else:\n    # Merge this page of results into the Delta table. Existing records that match on query_id have\n    # all their fields updated (needed because the status, end time, and error may change), and new\n    # records are inserted.\n    queries_table = DeltaTable.forName(spark, \"{0}.{1}\".format(DATABASE_NAME, QUERIES_TABLE_NAME))\n    queries_table.alias(\"queryResults\").merge(\n        query_results_clean.alias(\"newQueryResults\"),\n        \"queryResults.query_id = newQueryResults.query_id\") \\\n      .whenMatchedUpdateAll() \\\n      .whenNotMatchedInsertAll() \\\n      .execute()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Fetch from Query History API","showTitle":true,"inputWidgets":{},"nuid":"a7b059bf-0fc2-4271-bab7-c982cfd9605f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Starting to fetch page 0\nRequest parameters: {&#39;max_results&#39;: 25000, &#39;filter_by&#39;: {&#39;query_start_time_range&#39;: {&#39;start_time_ms&#39;: 1639535375562.0078, &#39;end_time_ms&#39;: 1639621775562.177}}, &#39;include_metrics&#39;: True}\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Starting to fetch page 0\nRequest parameters: {&#39;max_results&#39;: 25000, &#39;filter_by&#39;: {&#39;query_start_time_range&#39;: {&#39;start_time_ms&#39;: 1639535375562.0078, &#39;end_time_ms&#39;: 1639621775562.177}}, &#39;include_metrics&#39;: True}\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["response = requests.get(BILLING_URL, headers=auth_header_basic)\nif response.status_code != 200:\n  raise Exception(response.text)\n\nbilling_results = spark.createDataFrame(pd.read_csv(io.StringIO(response.content.decode('utf-8'))))\n\nbilling_results = billing_results.withColumn(\"timestamp\", billing_results[\"timestamp\"].cast(\"timestamp\")) \n#print (billing_results)\nif not check_table_exist(db_tbl_name=\"{0}.{1}\".format(DATABASE_NAME, BILLING_TABLE_NAME)):\n  # TODO: Probably makes sense to partition and/or Z-ORDER this table.\n  billing_results.write.format(\"delta\").saveAsTable(\"{0}.{1}\".format(DATABASE_NAME, BILLING_TABLE_NAME)) \nelse:\n  #Overwright table TODO MERGE LOGIC\n  spark.sql(\"DROP TABLE {0}.{1}\".format(DATABASE_NAME, BILLING_TABLE_NAME))\n  billing_results.write.format(\"delta\").saveAsTable(\"{0}.{1}\".format(DATABASE_NAME, BILLING_TABLE_NAME)) \n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Fetch Billing Data from API","showTitle":true,"inputWidgets":{},"nuid":"404f308e-615e-46aa-97b8-4f71ce387911"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;Response [200]&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;Response [200]&gt;\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(\"Time to execute: {}s\".format((current_time_in_millis() - notebook_start_execution_time) / 1000))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"End Process","showTitle":true,"inputWidgets":{},"nuid":"7b071db8-5408-4595-9a18-1fcd8ef41ccf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Time to execute: 84.298s\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Time to execute: 84.298s\n</div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"SpotApp_Fetch_API_Data_Public","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":535619816947431}},"nbformat":4,"nbformat_minor":0}
